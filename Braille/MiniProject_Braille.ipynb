{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1785d973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed line: caption\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosyd\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 3442272060_f9155194c2.jpg\n",
      "Generated Caption: two dogs are playing with each other in a field \n",
      "--------------------------------------------------\n",
      "Image: 482353373_03a9d5e8bc.jpg\n",
      "Generated Caption: a woman standing next to a man in a restaurant \n",
      "--------------------------------------------------\n",
      "Image: 947969010_f1ea572e89.jpg\n",
      "Generated Caption: a dog in the water with a frisbee in its mouth \n",
      "--------------------------------------------------\n",
      "Image: 1659358133_95cd1027bd.jpg\n",
      "Generated Caption: a black and white elephant jumping over a log \n",
      "--------------------------------------------------\n",
      "Image: 2646540383_343e1ec9a4.jpg\n",
      "Generated Caption: a man sitting on top of a surfboard in a pool \n",
      "--------------------------------------------------\n",
      "Image: 3351360323_91bb341350.jpg\n",
      "Generated Caption: a man in a suit and tie playing a guitar \n",
      "--------------------------------------------------\n",
      "Image: 2273028514_d7b584f73d.jpg\n",
      "Generated Caption: a woman is looking at her cell phone \n",
      "--------------------------------------------------\n",
      "Image: 1368338041_6b4077ca98.jpg\n",
      "Generated Caption: two dogs are standing next to each other on a sidewalk \n",
      "--------------------------------------------------\n",
      "Image: 1803631090_05e07cc159.jpg\n",
      "Generated Caption: a woman wearing a blue and white uniform \n",
      "--------------------------------------------------\n",
      "Image: 3422146099_35ffc8680e.jpg\n",
      "Generated Caption: a dog jumping up into the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 3018467501_a03d404413.jpg\n",
      "Generated Caption: a little girl and a little boy playing with a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 2467803152_70eeca1334.jpg\n",
      "Generated Caption: two dogs are playing in the woods together \n",
      "--------------------------------------------------\n",
      "Image: 2460823604_7f6f786b1c.jpg\n",
      "Generated Caption: a young boy in a baseball uniform catching a ball \n",
      "--------------------------------------------------\n",
      "Image: 3344411431_6f4917bb2f.jpg\n",
      "Generated Caption: a crowd of people standing next to each other \n",
      "--------------------------------------------------\n",
      "Image: 442594271_2c3dd38483.jpg\n",
      "Generated Caption: people on a beach flying kites \n",
      "--------------------------------------------------\n",
      "Image: 2439384468_58934deab6.jpg\n",
      "Generated Caption: two black and white dogs running through a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 2860041212_797afd6ccf.jpg\n",
      "Generated Caption: a black dog standing in a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 2162469360_ff777edc95.jpg\n",
      "Generated Caption: a man working on a pipe on top of a building \n",
      "--------------------------------------------------\n",
      "Image: 1526181215_c1a94325ae.jpg\n",
      "Generated Caption: a dog jumping in the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 3396157719_6807d52a81.jpg\n",
      "Generated Caption: a large brown and white dog standing on a rail \n",
      "--------------------------------------------------\n",
      "Image: 511282305_dbab4bf4be.jpg\n",
      "Generated Caption: a man and a woman sitting on a red and white trolley \n",
      "--------------------------------------------------\n",
      "Image: 2624044128_641b38c0cf.jpg\n",
      "Generated Caption: two young girls walking down a dirt road \n",
      "--------------------------------------------------\n",
      "Image: 2281006675_fde04e93dd.jpg\n",
      "Generated Caption: a person riding a surfboard on top of a beach \n",
      "--------------------------------------------------\n",
      "Image: 2814028429_561a215259.jpg\n",
      "Generated Caption: a man riding a raft on top of a body of water \n",
      "--------------------------------------------------\n",
      "Image: 3559781965_d4ec00e506.jpg\n",
      "Generated Caption: a dog running with a frisbee in its mouth \n",
      "--------------------------------------------------\n",
      "Image: 3677860841_3aa9d8036c.jpg\n",
      "Generated Caption: a woman is jumping in the water with a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 2263655670_517890f5b7.jpg\n",
      "Generated Caption: a dog running in the grass with a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 3038941104_17ee91fc03.jpg\n",
      "Generated Caption: a man on a skateboard doing a trick \n",
      "--------------------------------------------------\n",
      "Image: 2140182410_8e2a06fbda.jpg\n",
      "Generated Caption: a man laying in the snow with a snowboard \n",
      "--------------------------------------------------\n",
      "Image: 2934573544_7ffe92a2c9.jpg\n",
      "Generated Caption: a black dog running through a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 3606846822_28c40b933a.jpg\n",
      "Generated Caption: a man flying through the air while riding a skateboard \n",
      "--------------------------------------------------\n",
      "Image: 2136992638_098d62a3c5.jpg\n",
      "Generated Caption: two dogs standing next to each other on a sidewalk \n",
      "--------------------------------------------------\n",
      "Image: 497579819_f91b26f7d3.jpg\n",
      "Generated Caption: a brown and white dog standing in a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 3192069971_83c5a90b4c.jpg\n",
      "Generated Caption: a little boy with a helmet on riding a skateboard \n",
      "--------------------------------------------------\n",
      "Image: 252846811_7b250935a7.jpg\n",
      "Generated Caption: a brown dog and a brown dog are playing in the grass \n",
      "--------------------------------------------------\n",
      "Image: 2053733930_e245615ad4.jpg\n",
      "Generated Caption: a pair of skis sitting on top of a snow covered slope \n",
      "--------------------------------------------------\n",
      "Image: 3087095548_6df7c2a8ed.jpg\n",
      "Generated Caption: people are crossing a bridge over a river \n",
      "--------------------------------------------------\n",
      "Image: 3428038648_993a453f9e.jpg\n",
      "Generated Caption: a little boy holding a teddy bear in his mouth \n",
      "--------------------------------------------------\n",
      "Image: 3563673070_71fa0903ed.jpg\n",
      "Generated Caption: a man on a paddle board in the water \n",
      "--------------------------------------------------\n",
      "Image: 2359784186_36c9746d02.jpg\n",
      "Generated Caption: a woman holding a teddy bear next to a fire hydrant \n",
      "--------------------------------------------------\n",
      "Image: 3484832904_08619300d9.jpg\n",
      "Generated Caption: a young boy swinging a baseball bat at a ball \n",
      "--------------------------------------------------\n",
      "Image: 3300679815_2c6c2301cb.jpg\n",
      "Generated Caption: a variety of stuffed animals are displayed in a display \n",
      "--------------------------------------------------\n",
      "Image: 3567214106_6ece483f8b.jpg\n",
      "Generated Caption: a dog running through the grass with a frisbee in its mouth \n",
      "--------------------------------------------------\n",
      "Image: 3105929913_94a6882e25.jpg\n",
      "Generated Caption: three men in suits are sitting on a bed \n",
      "--------------------------------------------------\n",
      "Image: 2641288004_30ce961211.jpg\n",
      "Generated Caption: a black and white dog laying on top of a bed \n",
      "--------------------------------------------------\n",
      "Image: 1361420539_e9599c60ae.jpg\n",
      "Generated Caption: a small brown dog standing in a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 582899605_d96f9201f1.jpg\n",
      "Generated Caption: a man and a little girl sitting on the grass \n",
      "--------------------------------------------------\n",
      "Image: 2864340145_d28b842faf.jpg\n",
      "Generated Caption: a black dog swimming in a body of water \n",
      "--------------------------------------------------\n",
      "Image: 2877088081_7ca408cb25.jpg\n",
      "Generated Caption: a man riding a skateboard down the side of a cement wall \n",
      "--------------------------------------------------\n",
      "Image: 374176648_ba4b88c221.jpg\n",
      "Generated Caption: a man walking across a lush green hillside \n",
      "--------------------------------------------------\n",
      "Image: 3340575518_137ce2695f.jpg\n",
      "Generated Caption: people sitting on top of a hill \n",
      "--------------------------------------------------\n",
      "Image: 2574509968_e4692ae169.jpg\n",
      "Generated Caption: a woman standing on a sidewalk holding a cell phone \n",
      "--------------------------------------------------\n",
      "Image: 3650188378_cc8aea89f0.jpg\n",
      "Generated Caption: a man in a suit and tie standing next to a woman \n",
      "--------------------------------------------------\n",
      "Image: 2505988632_9541f15583.jpg\n",
      "Generated Caption: a little girl jumping in the air on a skateboard \n",
      "--------------------------------------------------\n",
      "Image: 3094317837_b31cbf969e.jpg\n",
      "Generated Caption: a man riding a wave on top of a surfboard \n",
      "--------------------------------------------------\n",
      "Image: 2685752892_9d5cd7f274.jpg\n",
      "Generated Caption: a man riding a skateboard on top of a fountain \n",
      "--------------------------------------------------\n",
      "Image: 3031263767_2e3856130e.jpg\n",
      "Generated Caption: a woman holding a surfboard in the water \n",
      "--------------------------------------------------\n",
      "Image: 415657941_454d370721.jpg\n",
      "Generated Caption: a little girl sitting on a bench in a park \n",
      "--------------------------------------------------\n",
      "Image: 2358554995_54ed3baa83.jpg\n",
      "Generated Caption: a little girl riding on top of a sled \n",
      "--------------------------------------------------\n",
      "Image: 2098646162_e3b3bbf14c.jpg\n",
      "Generated Caption: a black and white dog drinking water from a bottle \n",
      "--------------------------------------------------\n",
      "Image: 2698487246_e827404cac.jpg\n",
      "Generated Caption: young men playing a game of frisbee \n",
      "--------------------------------------------------\n",
      "Image: 3635577874_48ebaac734.jpg\n",
      "Generated Caption: a person jumping a skateboard in the air \n",
      "--------------------------------------------------\n",
      "Image: 751737218_b89839a311.jpg\n",
      "Generated Caption: a young man holding a skateboard next to another young man \n",
      "--------------------------------------------------\n",
      "Image: 1119418776_58e4b93eac.jpg\n",
      "Generated Caption: a black and white dog running through a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 3393035454_2d2370ffd4.jpg\n",
      "Generated Caption: a man on a skateboard in the woods \n",
      "--------------------------------------------------\n",
      "Image: 445655284_c29e6d7323.jpg\n",
      "Generated Caption: a dog jumping up into the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 1187435567_18173c148b.jpg\n",
      "Generated Caption: a dog that is laying down on a bed \n",
      "--------------------------------------------------\n",
      "Image: 2136455112_202c093ba4.jpg\n",
      "Generated Caption: a man is holding a skateboard in the middle of a room \n",
      "--------------------------------------------------\n",
      "Image: 2102581664_5ea50f85c6.jpg\n",
      "Generated Caption: a dog jumping up into the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 533602654_9edc74385d.jpg\n",
      "Generated Caption: two young boys sitting on top of a skateboard \n",
      "--------------------------------------------------\n",
      "Image: 3000722396_1ae2e976c2.jpg\n",
      "Generated Caption: a black and white dog with a frisbee in its mouth \n",
      "--------------------------------------------------\n",
      "Image: 2521878609_146143708e.jpg\n",
      "Generated Caption: a young man is biting into a piece of food \n",
      "--------------------------------------------------\n",
      "Image: 2616009069_82561da2e5.jpg\n",
      "Generated Caption: two dogs are standing in a grassy field \n",
      "--------------------------------------------------\n",
      "Image: 2774430374_fee1d793e7.jpg\n",
      "Generated Caption: a young boy holding a baseball bat on top of a dirt field \n",
      "--------------------------------------------------\n",
      "Image: 2939007933_8a6ef2d073.jpg\n",
      "Generated Caption: a person riding a motorcycle on top of a snow covered ground \n",
      "--------------------------------------------------\n",
      "Image: 3259119085_21613b69df.jpg\n",
      "Generated Caption: a man laying in the snow with a snowboard \n",
      "--------------------------------------------------\n",
      "Image: 424506167_01f365726b.jpg\n",
      "Generated Caption: a brown dog and a black dog playing in a field \n",
      "--------------------------------------------------\n",
      "Image: 2760715910_87c7bdeb87.jpg\n",
      "Generated Caption: a man in a red shirt is on a cell phone \n",
      "--------------------------------------------------\n",
      "Image: 1287931016_fb015e2e10.jpg\n",
      "Generated Caption: a dog standing next to a black and white dog \n",
      "--------------------------------------------------\n",
      "Image: 3546474710_903c3c9fd3.jpg\n",
      "Generated Caption: a man on a stage performing a trick on a guitar \n",
      "--------------------------------------------------\n",
      "Image: 3545427060_c16a8b7dfd.jpg\n",
      "Generated Caption: a man and a boy are playing in the water \n",
      "--------------------------------------------------\n",
      "Image: 859620561_de417cac1e.jpg\n",
      "Generated Caption: a dog jumping in the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 2338627102_6708a9b4fd.jpg\n",
      "Generated Caption: a man standing on top of a rock with a backpack \n",
      "--------------------------------------------------\n",
      "Image: 977856234_0d9caee7b2.jpg\n",
      "Generated Caption: a dog jumping in the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 3468346269_9d162aacfe.jpg\n",
      "Generated Caption: a car that is going down the road with smoke coming out of it \n",
      "--------------------------------------------------\n",
      "Image: 937559727_ae2613cee5.jpg\n",
      "Generated Caption: a man in a blue shirt is holding a fish in his hand \n",
      "--------------------------------------------------\n",
      "Image: 1446053356_a924b4893f.jpg\n",
      "Generated Caption: young girls playing a game of soccer \n",
      "--------------------------------------------------\n",
      "Image: 428483413_b9370baf72.jpg\n",
      "Generated Caption: a man and a woman standing in front of a store \n",
      "--------------------------------------------------\n",
      "Image: 2312984882_bec7849e09.jpg\n",
      "Generated Caption: a man standing next to a woman holding a wii controller \n",
      "--------------------------------------------------\n",
      "Image: 3724487641_d2096f10e5.jpg\n",
      "Generated Caption: two young boys standing next to each other on a sidewalk \n",
      "--------------------------------------------------\n",
      "Image: 3067500667_0fce8f28d4.jpg\n",
      "Generated Caption: a man holding a baseball bat on top of a field \n",
      "--------------------------------------------------\n",
      "Image: 3051125715_db76cebd1e.jpg\n",
      "Generated Caption: a large rock sitting on top of a rocky beach \n",
      "--------------------------------------------------\n",
      "Image: 1984936420_3f3102132b.jpg\n",
      "Generated Caption: a man standing on top of a rock next to a river \n",
      "--------------------------------------------------\n",
      "Image: 478592803_f57cc9c461.jpg\n",
      "Generated Caption: two people walking down the street with umbrellas \n",
      "--------------------------------------------------\n",
      "Image: 2428751994_88a6808246.jpg\n",
      "Generated Caption: a black and white dog with a frisbee in its mouth \n",
      "--------------------------------------------------\n",
      "Image: 379006645_b9a2886b51.jpg\n",
      "Generated Caption: a dog sitting on a ledge looking out \n",
      "--------------------------------------------------\n",
      "Image: 543326592_70bd4d8602.jpg\n",
      "Generated Caption: a crowd of people standing around a man in a suit \n",
      "--------------------------------------------------\n",
      "Image: 2315418282_80bd0bb1c0.jpg\n",
      "Generated Caption: a dog jumping up into the air to catch a frisbee \n",
      "--------------------------------------------------\n",
      "Image: 2830309113_c79d7be554.jpg\n",
      "Generated Caption: a young boy standing on top of a wooden bench \n",
      "--------------------------------------------------\n",
      "Image: 495033548_bd320405d8.jpg\n",
      "Generated Caption: young men playing a game of soccer \n",
      "--------------------------------------------------\n",
      "\n",
      "BLEU Score: 0.04\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Load the model, processor, and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Model configuration\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.max_length = 16\n",
    "model.config.num_beams = 4\n",
    "\n",
    "image_directory = \"Images/\"\n",
    "ground_truth_file = \"captions.txt/captions_8k.txt\"\n",
    "\n",
    "ground_truth_captions = {}\n",
    "try:\n",
    "    with open(ground_truth_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split based on the first comma\n",
    "            if \",\" in line:\n",
    "                image_name, caption = line.strip().split(\",\", 1)\n",
    "                ground_truth_captions[image_name] = caption\n",
    "            else:\n",
    "                print(f\"Skipping malformed line: {line.strip()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ground truth captions file not found: {ground_truth_file}\")\n",
    "    exit()\n",
    "\n",
    "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "random_images = random.sample(image_files, min(100, len(image_files)))\n",
    "\n",
    "generated_captions = []\n",
    "reference_captions = []\n",
    "\n",
    "for image_file in random_images:\n",
    "    image_path = os.path.join(image_directory, image_file)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        outputs = model.generate(inputs)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        print(f\"Image: {image_file}\")\n",
    "        print(f\"Generated Caption: {caption}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        generated_captions.append(caption)\n",
    "\n",
    "        if image_file in ground_truth_captions:\n",
    "            reference_captions.append([ground_truth_captions[image_file].split()])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_file}: {e}\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "if generated_captions and reference_captions:\n",
    "    bleu_score = corpus_bleu(reference_captions, [cap.split() for cap in generated_captions])\n",
    "    print(f\"\\nBLEU Score: {bleu_score:.2f}\")\n",
    "else:\n",
    "    print(\"No captions generated or ground-truth captions available for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8c2af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 3674521435_89ff681074.jpg\n",
      "Generated Caption: a man flying through the air while riding a skateboard \n",
      "--------------------------------------------------\n",
      "Image: 2483993827_243894a4f9.jpg\n",
      "Generated Caption: a man riding on the back of a white horse \n",
      "--------------------------------------------------\n",
      "Image: 1207159468_425b902bfb.jpg\n",
      "Generated Caption: a man standing on top of a snow covered slope \n",
      "--------------------------------------------------\n",
      "Image: 3098707588_5096d20397.jpg\n",
      "Generated Caption: two men in suits posing for a picture \n",
      "--------------------------------------------------\n",
      "Image: 2273105617_7c73d2d2d3.jpg\n",
      "Generated Caption: a man that is standing in front of a door \n",
      "--------------------------------------------------\n",
      "Image: 2213113526_beeb4f9bdc.jpg\n",
      "Generated Caption: a woman sitting on a bench next to a dog \n",
      "--------------------------------------------------\n",
      "Image: 528498076_43f0ef36b5.jpg\n",
      "Generated Caption: a young boy standing on top of a bed \n",
      "--------------------------------------------------\n",
      "Image: 2720985888_8f5920e8cf.jpg\n",
      "Generated Caption: a little girl sitting in the back of a truck \n",
      "--------------------------------------------------\n",
      "Image: 2342478660_faef1afea8.jpg\n",
      "Generated Caption: a young boy with a tooth brush in his mouth \n",
      "--------------------------------------------------\n",
      "Image: 3239866450_3f8cfb0c83.jpg\n",
      "Generated Caption: a man flying through the air while riding a skateboard \n",
      "--------------------------------------------------\n",
      "\n",
      "BLEU Score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import string\n",
    "\n",
    "# Load the model, processor, and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Update model configuration\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.max_length = 32  \n",
    "model.config.num_beams = 10  \n",
    "model.config.early_stopping = True\n",
    "\n",
    "image_directory = \"Images/\"\n",
    "ground_truth_file = \"captions_8k.txt\"\n",
    "\n",
    "ground_truth_captions = {}\n",
    "with open(ground_truth_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        if \",\" in line:\n",
    "            image_name, caption = line.strip().split(\",\", 1)\n",
    "            ground_truth_captions[image_name] = caption.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "image_files = [f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "random_images = random.sample(image_files, min(10, len(image_files)))\n",
    "\n",
    "# Generate captions and prepare for BLEU score\n",
    "generated_captions = []\n",
    "reference_captions = []\n",
    "\n",
    "for image_file in random_images:\n",
    "    image_path = os.path.join(image_directory, image_file)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        outputs = model.generate(inputs)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True).lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        print(f\"Image: {image_file}\")\n",
    "        print(f\"Generated Caption: {caption}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        generated_captions.append(caption.split())\n",
    "\n",
    "        if image_file in ground_truth_captions:\n",
    "            reference_captions.append([ground_truth_captions[image_file].split()])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_file}: {e}\")\n",
    "\n",
    "# Calculate BLEU score\n",
    "if generated_captions and reference_captions:\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(reference_captions, generated_captions, smoothing_function=smoothing_function)\n",
    "    print(f\"\\nBLEU Score: {bleu_score*10:.2f}\")\n",
    "else:\n",
    "    print(\"No captions generated or ground-truth captions available for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb7e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Python312\\Lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (4.47.1)\n",
      "Requirement already satisfied: torch in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (0.20.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python312\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python312\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rosyd\\appdata\\roaming\\python\\python312\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python312\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python312\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests->transformers) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rosyd\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:1527: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption for imgg3.jpg: a green plastic bottle sitting on top of a wooden table \n",
      "Braille Output: ⠁ ⠛⠗⠑⠑⠝ ⠏⠇⠁⠎⠞⠊⠉ ⠃⠕⠞⠞⠇⠑ ⠎⠊⠞⠞⠊⠝⠛ ⠕⠝ ⠞⠕⠏ ⠕⠋ ⠁ ⠺⠕⠕⠙⠑⠝ ⠞⠁⠃⠇⠑ \n"
     ]
    }
   ],
   "source": [
    "# %pip install transformers torch torchvision pillow\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Configure the model\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.max_length = 16\n",
    "model.config.num_beams = 4\n",
    "\n",
    "braille_dict = {\n",
    "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑',\n",
    "    'f': '⠋', 'g': '⠛', 'h': '⠓', 'i': '⠊', 'j': '⠚',\n",
    "    'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕',\n",
    "    'p': '⠏', 'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞',\n",
    "    'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭', 'y': '⠽', 'z': '⠵',\n",
    "    ' ': ' ', '1': '⠼⠁', '2': '⠼⠃', '3': '⠼⠉', '4': '⠼⠙',\n",
    "    '5': '⠼⠑', '6': '⠼⠋', '7': '⠼⠛', '8': '⠼⠓', '9': '⠼⠊', '0': '⠼⠚'\n",
    "}\n",
    "\n",
    "def text_to_braille(text):\n",
    "    return ''.join(braille_dict.get(char.lower(), '?') for char in text)\n",
    "\n",
    "def generate_caption_and_braille(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        image.show()\n",
    "      \n",
    "        outputs = model.generate(inputs)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        braille_text = text_to_braille(caption)\n",
    "        \n",
    "        print(f\"Generated Caption for {os.path.basename(image_path)}: {caption}\")\n",
    "        print(f\"Braille Output: {braille_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the image {os.path.basename(image_path)}: {e}\")\n",
    "\n",
    "local_image_path = input(\"Enter the relative or absolute path to the image (e.g., 'image.jpg'): \")\n",
    "\n",
    "generate_caption_and_braille(local_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ebbf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Processing Batches: 100%|██████████| 1265/1265 [00:01<00:00, 881.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to generated_captions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "\n",
    "# Initialize model, processor, and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, captions_file, image_dir, processor):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.captions_df = pd.read_csv(captions_file, names=[\"filename\", \"caption\"])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.captions_df.iloc[idx]\n",
    "        image_path = os.path.normpath(os.path.join(self.image_dir, row['filename']))\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "            caption = row['caption']\n",
    "            return pixel_values.squeeze(0), caption\n",
    "        except Exception as e:\n",
    "            return None, None\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    return [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "\n",
    "def generate_captions(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "            valid_batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "            if not valid_batch:\n",
    "                continue\n",
    "            \n",
    "            images, captions = zip(*valid_batch)\n",
    "            images = torch.stack(images).to(device)\n",
    "            outputs = model.generate(images, max_length=64, num_beams=4)\n",
    "            generated_captions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            \n",
    "            for orig_cap, gen_cap in zip(captions, generated_captions):\n",
    "                all_results.append({\n",
    "                    \"generated_caption\": gen_cap,\n",
    "                    \"original_caption\": orig_cap\n",
    "                })\n",
    "    return all_results\n",
    "\n",
    "captions_file = r\"captions_8k.txt\"\n",
    "image_dir = r\"C:\\Users\\rosyd\\Mini_Project\\Images\"\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImageCaptionDataset(captions_file, image_dir, processor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "results = generate_captions(model, dataloader, tokenizer)\n",
    "\n",
    "output_csv = \"generated_captions.csv\"\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea759504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"architectures\": [\n",
      "    \"ViTModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.47.1\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'> is overwritten by shared decoder config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50256,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.47.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Processing Batches: 100%|██████████| 1265/1265 [00:01<00:00, 947.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to generated_captions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n",
    "import subprocess\n",
    "\n",
    "# Initialize model, processor, and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\").to(device)\n",
    "processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "\n",
    "# Dataset class\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, captions_file, image_dir, processor):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.captions_df = pd.read_csv(captions_file, names=[\"filename\", \"caption\"])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.captions_df.iloc[idx]\n",
    "        image_path = os.path.normpath(os.path.join(self.image_dir, row['filename']))\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            # Load and preprocess the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "            caption = row['caption']\n",
    "            return pixel_values.squeeze(0), caption\n",
    "        except Exception as e:\n",
    "            # Skip problematic images\n",
    "            return None, None\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    return [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "\n",
    "def generate_captions(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    all_results = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Processing Batches\"):\n",
    "            valid_batch = [(img, cap) for img, cap in batch if img is not None and cap is not None]\n",
    "            if not valid_batch:\n",
    "                continue\n",
    "            \n",
    "            images, captions = zip(*valid_batch)\n",
    "            images = torch.stack(images).to(device)\n",
    "            outputs = model.generate(images, max_length=64, num_beams=4)\n",
    "            generated_captions = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "            \n",
    "            for orig_cap, gen_cap in zip(captions, generated_captions):\n",
    "                all_results.append({\n",
    "                    \"generated_caption\": gen_cap,\n",
    "                    \"original_caption\": orig_cap\n",
    "                })\n",
    "    return all_results\n",
    "\n",
    "captions_file = r\"captions_8k.txt\"\n",
    "image_dir = r\"C:\\Users\\rosyd\\Mini_Project\\Images\"\n",
    "\n",
    "dataset = ImageCaptionDataset(captions_file, image_dir, processor)\n",
    "subprocess.run([\"python\", \"changes.py\"])\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "results = generate_captions(model, dataloader, tokenizer)\n",
    "\n",
    "output_csv = \"generated_captions.csv\"\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962f30dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save model, processor, and tokenizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moo.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump({\n\u001b[1;32m----> 6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmodel\u001b[49m,\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m: processor,\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: tokenizer\n\u001b[0;32m      9\u001b[0m     }, f)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel, processor, and tokenizer saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moo.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model, processor, and tokenizer\n",
    "with open(\"oo.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"model\": model,\n",
    "        \"processor\": processor,\n",
    "        \"tokenizer\": tokenizer\n",
    "    }, f)\n",
    "\n",
    "print(\"Model, processor, and tokenizer saved to 'oo.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22b03c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flask in c:\\python312\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\python312\\lib\\site-packages (from flask) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\python312\\lib\\site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\python312\\lib\\site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\python312\\lib\\site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\python312\\lib\\site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click>=8.1.3->flask) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c544693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "try:\n",
    "    with open(\"oo.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .pkl file: {e}\")\n",
    "    sys.exit(1)\n",
    "print(\"Model, processor, and tokenizer loaded successfully.\")\n",
    "\n",
    "braille_dict = {\n",
    "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑',\n",
    "    'f': '⠋', 'g': '⠛', 'h': '⠓', 'i': '⠊', 'j': '⠚',\n",
    "    'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕',\n",
    "    'p': '⠏', 'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞',\n",
    "    'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭', 'y': '⠽', 'z': '⠵',\n",
    "    ' ': ' ', '1': '⠼⠁', '2': '⠼⠃', '3': '⠼⠉', '4': '⠼⠙',\n",
    "    '5': '⠼⠑', '6': '⠼⠋', '7': '⠼⠛', '8': '⠼⠓', '9': '⠼⠊', '0': '⠼⠚'\n",
    "}\n",
    "\n",
    "def text_to_braille(text):\n",
    "    return ''.join(braille_dict.get(char.lower(), '?') for char in text)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\",methods=['GET'])\n",
    "def home():\n",
    "    return \"Welcome to the Image Captioning and Braille Generation API!\"\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_caption_and_braille():\n",
    "    try:\n",
    "        data = request.json\n",
    "        image_data = base64.b64decode(data['image'])\n",
    "        image = Image.open(BytesIO(image_data))\n",
    "        \n",
    "        inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        \n",
    "        outputs = model.generate(inputs)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        braille_text = text_to_braille(caption)\n",
    "        \n",
    "        return jsonify({\n",
    "            \"caption\": caption,\n",
    "            \"braille\": braille_text\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,port=5001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe2e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, processor, and tokenizer loaded successfully.\n",
      "Generated Caption for imgg2.jpg: a green book is sitting on top of a wooden table \n",
      "Braille Output: ⠁ ⠛⠗⠑⠑⠝ ⠃⠕⠕⠅ ⠊⠎ ⠎⠊⠞⠞⠊⠝⠛ ⠕⠝ ⠞⠕⠏ ⠕⠋ ⠁ ⠺⠕⠕⠙⠑⠝ ⠞⠁⠃⠇⠑ \n"
     ]
    }
   ],
   "source": [
    "with open(\"oo.pkl\", \"rb\") as f:\n",
    "\n",
    "    data = pickle.load(f)\n",
    "    #print(data[\"model\"])\n",
    "    model = data[\"model\"]\n",
    "    tokenizer = data[\"tokenizer\"]\n",
    "print(\"Model, processor, and tokenizer loaded successfully.\")\n",
    "\n",
    "\n",
    "def generate_caption_and_braille(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "        image.show()\n",
    "        \n",
    "        outputs = model.generate(inputs)\n",
    "        caption = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        braille_text = text_to_braille(caption)\n",
    "        \n",
    "        print(f\"Generated Caption for {os.path.basename(image_path)}: {caption}\")\n",
    "        print(f\"Braille Output: {braille_text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the image {os.path.basename(image_path)}: {e}\")\n",
    "\n",
    "generate_caption_and_braille(\"imgg2.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e59042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
